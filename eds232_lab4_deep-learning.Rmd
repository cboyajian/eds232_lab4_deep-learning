---
title: 'Lab 4: Deep Learning - iNaturalist'
author: "Clarissa Boyajian"
date: "2/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(digest, tidyverse, DT, tensorflow, keras, glue, here)
```

# Create directories and images for 2 species and 10 species models

```{r}
# path to folder containing species directories of images
dir_src  <- "/courses/EDS232/inaturalist-2021/train_mini"
dir_dest <- here("inat/")
dir.create(dir_dest, showWarnings = F)

# get list of directories, one per species (n = 10,000 species)
dirs_spp <- list.dirs(dir_src, recursive = F, full.names = T)
n_spp <- length(dirs_spp)

# set seed (for reproducible results) 
# just before sampling (otherwise get different results)
# based on your username (unique amongst class)
Sys.info()[["user"]] %>% 
  digest::digest2int() %>% 
  set.seed()
i10 <- sample(1:n_spp, 10)

# show the 10 indices sampled of the 10,000 possible 
i10

# show the 10 species directory names
basename(dirs_spp)[i10]
```

```{r}
# show the first 2 species directory names
i2 <- i10[1:2]
basename(dirs_spp)[i2]
```


```{r}
# setup data frame with source (src) and destination (dest) paths to images
d <- tibble(
  set     = c(rep("spp2", 2), rep("spp10", 10)),
  dir_sp  = c(dirs_spp[i2], dirs_spp[i10]),
  tbl_img = map(dir_sp, function(dir_sp){
    tibble(
      src_img = list.files(dir_sp, full.names = T),
      subset  = c(rep("train", 30), rep("validation", 10), rep("test", 10))) })) %>% 
  unnest(tbl_img) %>% 
  mutate(
    sp       = basename(dir_sp),
    img      = basename(src_img),
    dest_img = glue("{dir_dest}/{set}/{subset}/{sp}/{img}"))

# show source and destination for first 10 rows of tibble
d %>% 
  select(src_img, dest_img)
```

```{r}
# iterate over rows, creating directory if needed and copying files 
d %>% 
  pwalk(function(src_img, dest_img, ...){
    dir.create(dirname(dest_img), recursive = T, showWarnings = F)
    file.copy(src_img, dest_img) })
```

# 2 Species

## Pre-processing

```{r}
## pre-processing ##
train_dir_2sp <- here("inat/spp2/train/")
validation_dir_2sp <- here("inat/spp2/validation/")
test_dir_2sp <- here("inat/spp2/test/")

## all images will be rescaled by 1/255 ##
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)
test_datagen <- image_data_generator(rescale = 1/255)

train_generator_2sp <- flow_images_from_directory(
  train_dir_2sp, # target directory
  train_datagen, # data generator
  target_size = c(150, 150), # all images resized to 150x150
  batch_size = 5, # larger than 5 throws error
  class_mode = "binary" # used binary_crossentropy loss, need binary labels
  )

validation_generator_2sp <- flow_images_from_directory(
  validation_dir_2sp,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary")

test_generator_2sp <- flow_images_from_directory(
  test_dir_2sp,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary")
```

## Neural net model

```{r}
# model using neural nets
model_nn_2sp <- keras_model_sequential() %>% 
  layer_flatten() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units =  1, activation = "sigmoid")
```

```{r}
# compiling model
model_nn_2sp %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("accuracy"))
```

```{r}
# fitting model using neural nets
history_nn_2sp <- model_nn_2sp %>% fit(
    train_generator_2sp,
    steps_per_epoch = 6,
    epochs = 11,
    validation_data = validation_generator_2sp,
    validation_steps = 1)
```

```{r}
plot(history_nn_2sp)
```
```{r}
history_nn_2sp
```

```{r}
results_nn_2sp <- model_nn_2sp %>% evaluate(test_generator_2sp)

results_nn_2sp
```


## Convolutional neural net model

```{r}
# model using convolutional neural net
model_cnn_2sp <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
# compiling model cnn
model_cnn_2sp %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("accuracy"))
```

```{r}
# fitting model cnn
history_cnn_2sp <- model_cnn_2sp %>% fit_generator(
    train_generator_2sp,
    steps_per_epoch = 5,
    epochs = 11,
    validation_data = validation_generator_2sp,
    validation_steps = 1)
```

```{r}
plot(history_cnn_2sp)
```
```{r}
history_cnn_2sp
```

```{r}
results_cnn_2sp <- model_cnn_2sp %>% evaluate(test_generator_2sp)

results_cnn_2sp
```

## Standard neural net vs. convolutional neural net

```{r}
results_nn_2sp

results_cnn_2sp
```

The standard neural net model when evaluated on the test images has an accuracy rate of `r results_nn_2sp[[2]]` and a loss raet of `r results_nn_2sp[[1]]` as compared to the convolutional neural net model which has an accuracy rate of `r results_cnn_2sp[[2]]` and loss rate of `r results_cnn_2sp[[1]]`. Based on this, I would choose to use the convolutional neural net.



# 10 species 

## Pre-processing

```{r}
## pre-processing ##
train_dir_10sp <- here("inat/spp10/train/")
validation_dir_10sp <- here("inat/spp10/validation/")
test_dir_10sp <- here("inat/spp10/test/")

## all images will be rescaled by 1/255 ##
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)
test_datagen <- image_data_generator(rescale = 1/255)

train_generator_10sp <- flow_images_from_directory(
  train_dir_10sp, # target directory
  train_datagen, # data generator
  target_size = c(150, 150), # all images resized to 150x150
  batch_size = 5, # larger than 5 throws error
  class_mode = "categorical"
  )

validation_generator_10sp <- flow_images_from_directory(
  validation_dir_10sp,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical")

test_generator_10sp <- flow_images_from_directory(
  test_dir_10sp,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical")
```


## Neural net model

```{r}
# model using neural nets
model_nn_10sp <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_dense(units = 64, activation = "relu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 10, activation = "softmax") 
```

```{r}
# compile model
model_nn_10sp %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy"))
```

```{r}
# fitting model using neural nets
history_nn_10sp <- model_nn_10sp %>% fit(
    train_generator_10sp,
    steps_per_epoch = 5,
    epochs = 30,
    validation_data = validation_generator_10sp,
    validation_steps = 10)
```

```{r}
plot(history_nn_10sp)
```

```{r}
history_nn_10sp
```

```{r}
results_nn_10sp <- model_nn_10sp %>% evaluate(test_generator_10sp)

results_nn_10sp
```


## Convolutional neural net model

```{r}
# model using convolutional neural net
model_cnn_10sp <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")
```

```{r}
# compiling model cnn
model_cnn_10sp %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("accuracy"))
```

```{r}
# fitting model cnn
history_cnn_10sp <- model_cnn_10sp %>% fit_generator(
    train_generator_10sp,
    steps_per_epoch = 5,
    epochs = 30,
    validation_data = validation_generator_10sp,
    validation_steps = 10)
```

```{r}
plot(history_cnn_10sp)
```

```{r}
history_cnn_10sp
```

```{r}
results_cnn_10sp <- model_cnn_10sp %>% evaluate(test_generator_10sp)

results_cnn_10sp
```


## Standard neural net vs. convolutional neural net

```{r}
results_nn_10sp

results_cnn_10sp
```

The standard neural net model when evaluated on the test images has an accuracy rate of `r results_nn_10sp[[2]]` and a loss rate of `r results_nn_10sp[[1]]` as compared to the convolutional neural net model which has an accuracy rate of `r results_cnn_10sp[[2]]` and loss rate of `r results_cnn_10sp[[1]]`. Based on this, I would choose the standard neural net model.
